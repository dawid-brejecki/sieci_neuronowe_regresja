# -*- coding: utf-8 -*-
"""sieci_neuronowe_regresja.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q0AKNUviGUzqVG-NzucFFyT8UScYWh3H
"""

import numpy as np
import pandas as pd
from keras.utils.np_utils import to_categorical 
from keras import models
from keras import layers
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from keras.utils.vis_utils import model_to_dot
from IPython.display import SVG
import matplotlib.pyplot as plt
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras import regularizers

df=pd.read_csv('output.csv')

# wybranie tylko interesujacych kolumn ze zbioru
df = df.iloc[:,[4,6,7,8,9,10,11]]

# zmiana nazw kolumn dla czytelnosci
nowe_nazwy = ['happiness','economy','family','health','freedom','trust','social']
df.columns = nowe_nazwy

# z uwagi na dosc duza liczbe danych, w tym przypadku, usuwam wiersze z brakami, zamiast np. zastapienia ich srednia
df = df.dropna()

# przygotowanie X i Y
Y = df['happiness']
features = ['economy','family','freedom']
X = df[features]

# podzial zbioru na testowy i treningowy
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25)


scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)

scaler = StandardScaler()
X_test = scaler.fit_transform(X_test)

# budowanie modelu
network = models.Sequential()
# Dodanie warstwy wejsciowej z funkcją aktywacji ReLU
network.add(layers.Dense(units=3, activation='relu', input_shape=(3,)))
# zapobieganie przeuczeniu
network.add(layers.Dropout(0.2))
# Dodanie warstwy ukrytej z funkcją aktywacji ReLU.
network.add(layers.Dense(units=3, activation='relu', kernel_regularizer=regularizers.l2(0.01)))
# zapobieganie przeuczeniu
network.add(layers.Dropout(0.5))
# Dodanie warstwy wyjsciowej
network.add(layers.Dense(units=1, activation = 'linear'))
# Kompilacja sieci neuronowej. 
network.compile(loss='mse', optimizer='rmsprop', metrics=['mse'])
# Zakonczenie procesu uczenia w momencie zwiekszajacej sie straty zbioru testowego i zapis najlepszego modelu
callbacks = [EarlyStopping(monitor="val_loss", patience=2), ModelCheckpoint(filepath="best_model.h5",
monitor="val_loss", save_best_only=True)]
# Wytrenowanie sieci neuronowej.
history = network.fit(X_train, # Cechy.
                      Y_train, # Wektor docelowy.
                      epochs=300,
                      verbose=1, # opis
                      validation_data=(X_test, Y_test))

# predykcja
Y_pred = network.predict(X_test)
# wizualizacja funkcji straty
training_loss = history.history["loss"] 
test_loss = history.history["val_loss"]

epoch_count = range(1, len(training_loss) + 1)

plt.plot(epoch_count, training_loss, "r--")
plt.plot(epoch_count, test_loss, "b-")
plt.legend(["Strata zbioru uczącego", "Strata zbioru testowego"]) 
plt.xlabel("Epoka")
plt.ylabel("Strata")
plt.show()

# siec zostala poddana mocnej regularyzacji, stad troche nizsza dokladnosc